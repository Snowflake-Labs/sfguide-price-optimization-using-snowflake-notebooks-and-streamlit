{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "# Tasty Bytes - Price Optimization\n\n## Overview\n**Tasty Bytes is one of the largest food truck networks** in the world with localized menu options spread across  **15 food truck brands** globally. **Tasty Bytes is aiming to achieve 25% YoY sales growth over 5 years.**\n\n**Price optimization** enables Tasty Bytes to achieve this goal by **determining the right prices** for their menu items to **maximize profitability** while **maintaining customer satisfaction**.\n\nAs Tasty Bytes Data Scientists, **we will implement price optimization for their diversified food-truck brands to inform their pricing and promotions**. \n\nIn this notebook, we will train & deploy an ML model which leverages historical menu-item sale data to understand how menu-item demand changes with varying price. By utilizing this trained model, we would recommend the optimal day of week prices for all menu-items for the upcoming month to our food-truck brands.",
   "id": "6e897a7b-da6c-4597-9dd4-1bb2aa02a933"
  },
  {
   "metadata": {
    "name": "cell2"
   },
   "cell_type": "markdown",
   "source": [
    "**In this notebook, we will use Snowpark to:**\n",
    "\n",
    "- Explore the data\n",
    "- Perform feature engineering\n",
    "- Train a model\n",
    "- Deploy & utilize the model in Snowflake\n",
    "\n",
    "**Benefits of using Snowpark:**\n",
    "\n",
    "- No copies or movement of data\n",
    "- Maintain governance\n",
    "- Leverage Snowflake scalable compute"
   ],
   "id": "9c753406-0566-4a77-820a-7f0c34e1f563"
  },
  {
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "### Import Packages\nAfter installing Snowpark in our Python environment, we import the Snowpark packages similar to any other Python package.",
   "id": "5c7b4c8d-2d22-42d6-8574-73a2f4a10467"
  },
  {
   "cell_type": "code",
   "id": "e1b2c1a8-f61a-4d82-ac45-5f5e10feb0b5",
   "metadata": {
    "language": "python",
    "name": "import_libraries",
    "collapsed": false
   },
   "outputs": [],
   "source": "#Import required libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport cachetools\nimport xgboost\nimport ast\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Import viz packages \nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=10)\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\n\n# Import Snowflake modules\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark import Window\nimport snowflake.snowpark.functions as F\nimport snowflake.snowpark.types as T\nfrom snowflake.ml.registry.registry import Registry\nfrom snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection.grid_search_cv import GridSearchCV\n\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7075ef3-ad22-4b1e-91aa-86f2bc0cb93c",
   "metadata": {
    "language": "python",
    "name": "show_current_environment"
   },
   "outputs": [],
   "source": "# Current Environment Details\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell4"
   },
   "cell_type": "markdown",
   "source": [
    "# Data Exploration\n",
    "We will be conducting all our data exploration by leveraging Snowpark Dataframe enabling us to utilize Snowflake compute to do our operations while employing the familiar data representation of Pandas dataframes which is popular among Python users.\n",
    "\n",
    "### Snowpark DataFrame\n",
    "Let's create a Snowpark DataFrame by reading the data in the daily menu-item sale view in our Snowflake account using Snowpark's session.table function."
   ],
   "id": "d1ec4287-b4a5-4664-b1bf-951d83c38c95"
  },
  {
   "cell_type": "code",
   "id": "b1e06dde-597b-4115-8e8d-2f1ac4d7acb8",
   "metadata": {
    "language": "python",
    "name": "initializing_sales_daily_df"
   },
   "outputs": [],
   "source": "# Read daily sales data from snowflake table into Snowpark dataframe\nsales_daily_df = session.table(\"menu_item_aggregate_v\").order_by([\"TRUCK_BRAND_NAME\",\"MENU_ITEM_ID\",\"DATE\"])",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell5"
   },
   "cell_type": "markdown",
   "source": [
    "### Preview the Data\n",
    "With our Snowpark DataFrame defined, we use the .show() function and specify the number of rows we want printed to be 5. This value is defaulted to 10 in case the argument is not provided. "
   ],
   "id": "40b0b621-7604-4cfd-8474-de8df9c4150b"
  },
  {
   "cell_type": "code",
   "id": "b1e771fb-04a0-4720-a196-d73249edaf0f",
   "metadata": {
    "language": "python",
    "name": "show_sales_daily_df_rows"
   },
   "outputs": [],
   "source": "sales_daily_df.show(5)",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell6"
   },
   "cell_type": "markdown",
   "source": [
    "### How many rows are in our data?\n",
    "\n",
    "We utilize the Snowpark count function to how much data we are working with. It is similar to COUNT function in SQL. The rows are counted within Snowflake without any data transfer. "
   ],
   "id": "9f1bc594-025c-4844-b27d-92d4a1735592"
  },
  {
   "cell_type": "code",
   "id": "e097e0c4-0b97-4f90-93e7-41a50df83e9e",
   "metadata": {
    "language": "python",
    "name": "count_records_for_sales_daily_df"
   },
   "outputs": [],
   "source": "# Number of records in our data\nsales_daily_df.count() ",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell7"
   },
   "cell_type": "markdown",
   "source": [
    "### What are the columns in our data?\n",
    "Let's access the columns object of our Snowpark Dataframe to look at all the columns present in our data."
   ],
   "id": "92536bfb-e594-40c9-9910-65811382484b"
  },
  {
   "cell_type": "code",
   "id": "743cd907-3c94-4a11-b6fb-f486007d7772",
   "metadata": {
    "language": "python",
    "name": "show_sales_daily_df_columns"
   },
   "outputs": [],
   "source": "# Columns in our data\nsales_daily_df.columns",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell8"
   },
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In this section, we will build our feature engineering pipeline which leverages Snowflake compute to perform aggregation and transformation operations enabling us to utilize native Snowflake performance & scale."
   ],
   "id": "72b4a356-e0b8-4a7c-86db-9af8640a3671"
  },
  {
   "metadata": {
    "name": "cell9"
   },
   "cell_type": "markdown",
   "source": [
    "### Create Aggregate Table\n",
    "\n",
    "We create aggregate table by utilizing the Snowpark group_by function to define groups rows by the columns specified and Snowpark agg function to get average for our aggregated columns. "
   ],
   "id": "abd847c9-22ea-46ad-b48e-20dc45b949dc"
  },
  {
   "cell_type": "code",
   "id": "3c7e64de-badc-4b38-9a32-03b0d0dc6b78",
   "metadata": {
    "language": "python",
    "name": "initialize_sales_agg_df"
   },
   "outputs": [],
   "source": "# Create year & month column\nsales_daily_df = sales_daily_df.with_column(\"MONTH\",F.date_part(\"MONTH\", F.col(\"DATE\"))) \\\n                .with_column(\"YEAR\",F.date_part(\"YEAR\", F.col(\"DATE\")))\n\n# Group by columns : Truck_Brand_ID, Month, Weekday, Menu_Item_ID\n# Aggegrate columns : AVG(TOTAL_QUANTITY_SOLD), AVG(PRICE), AVG(COST_OF_GOODS_USD), AVG(\"BASE_PRICE\")\nsales_agg_df = sales_daily_df.group_by([\"TRUCK_BRAND_NAME\", \"MONTH\", \"YEAR\", \"DAY_OF_WEEK\",\"MENU_ITEM_ID\"]) \\\n                    .agg(F.avg(\"TOTAL_QUANTITY_SOLD\").alias(\"TOTAL_QUANTITY_SOLD\"), \\\n                         F.avg(\"PRICE\").alias(\"PRICE\"), \\\n                         F.avg(\"BASE_PRICE\").alias(\"BASE_PRICE\"), \\\n                         F.avg(\"COST_OF_GOODS_USD\").alias(\"COST_OF_GOODS_USD\"))\n\nsales_agg_df.show(5)",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell10"
   },
   "cell_type": "markdown",
   "source": [
    "### Create New Feature\n",
    "\n",
    "We track for price fluctuations by creating a new column and naming it Price Change by leveraging Snowpark with_column function."
   ],
   "id": "4a1353c8-f470-4325-8a66-fce51fc2c684"
  },
  {
   "cell_type": "code",
   "id": "cc17bdf6-bb6e-4d04-91d0-83068c8bd003",
   "metadata": {
    "language": "python",
    "name": "price_change_column"
   },
   "outputs": [],
   "source": "# Price change column\nsales_agg_df = sales_agg_df.with_column(\"PRICE_CHANGE\",(F.col(\"PRICE\") - F.col(\"BASE_PRICE\"))/ F.col(\"BASE_PRICE\"))\n",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell11"
   },
   "cell_type": "markdown",
   "source": [
    "### Rows for Future Month\n",
    "\n",
    "Next we add rows to our Snowpark Dataframe for the upcoming month. We do so by utilizing Snowpark functions enabling us to push all our complex data transformations onto our Snowflake compute.  "
   ],
   "id": "a5488bf0-2874-4d91-8871-b095173c3826"
  },
  {
   "cell_type": "code",
   "id": "ec2859de-53c8-4c4e-97c9-8d788689fc92",
   "metadata": {
    "language": "python",
    "name": "updating_sales_agg_df"
   },
   "outputs": [],
   "source": "## Future month\n# Get latest month & year\nmax_year = sales_agg_df.select(F.max(\"YEAR\")).collect()[0][0]\nmax_month = (\n    sales_agg_df.filter(F.col(\"YEAR\") == max_year)\n    .select(F.max(\"MONTH\"))\n    .collect()[0][0]\n)\n\n# Get future month\nnew_year = max_year if max_month < 12 else max_year + 1\nnew_month = max_month + 1 if max_month < 12 else 1\n\n# Create future dataframe\nfuture_df = session.create_dataframe([0,1, 2, 3, 4, 5, 6]).to_df(\"DAY_OF_WEEK\")\nfuture_df = future_df.with_columns(\n        [\"MONTH\", \"YEAR\", \"TOTAL_QUANTITY_SOLD\", \"PRICE\",\"PRICE_CHANGE\"],\n        [F.lit(new_month), F.lit(new_year), F.lit(None), F.lit(0),  F.lit(0)],\n    )\n\n# Get menu_item_id, truck_brand, base_price & cost_of_goods \nmenu_df = session.table(\"menu_item_cogs_and_price_v\") \\\n                 .with_column(\"YEAR\",F.date_part(\"YEAR\", F.col(\"START_DATE\"))).filter(F.col(\"YEAR\") == new_year) \\\n                 .select([\"MENU_ITEM_ID\",\"COST_OF_MENU_ITEM_USD\",\"SALES_PRICE_USD\"]).distinct() \\\n                 .with_column_renamed(F.col(\"SALES_PRICE_USD\"),\"BASE_PRICE\") \\\n                 .with_column_renamed(F.col(\"COST_OF_MENU_ITEM_USD\"),\"COST_OF_GOODS_USD\")\n\ntruck_brand_df = sales_agg_df.select([\"MENU_ITEM_ID\",\"TRUCK_BRAND_NAME\"]).distinct()\nmenu_df = menu_df.join(truck_brand_df, (menu_df.col(\"MENU_ITEM_ID\") == truck_brand_df.col(\"MENU_ITEM_ID\")) , \\\n                                        how = \"inner\" , lsuffix=\"_left\", _rsuffix=\"_right\") \\\n                                  .drop([\"MENU_ITEM_ID_left\"])\n\n# Cross-join with future dataframe \nfuture_df = future_df.cross_join(menu_df)\n\n# Append to historical agg sales data\nsales_agg_df = future_df.union_all_by_name(sales_agg_df)",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "### Rolling Aggregate Features\n\nWe will use a Snowflake window function to get a **rolling averages of price & price change columns** over time. Window functions allow us to aggregate on a \"moving\" group of rows.\n\nWe create **two partitions** for rolling averages over **varied lags** (last month, last year, historical):\n1. Menu-item & Day of Week \n2. Menu-item\n\nWe create 6 rolling features in total.",
   "id": "ea1b8e2a-4a8b-4a8d-ac2a-d827e4796c63"
  },
  {
   "cell_type": "code",
   "id": "fc12b35b-12bb-49de-b53f-93b415556bd2",
   "metadata": {
    "language": "python",
    "name": "define_window_function"
   },
   "outputs": [],
   "source": "# Define function to create Window Average Columns for Snowpark Dataframe based on given column & lag\ndef create_window_columns(df, col_name, agg_col, lag, partition_col, order_col, lag_param):\n    if lag == \"HIST\" : \n        window = (\n        Window.partition_by(partition_col) \\\n        .order_by(order_col) \\\n        .rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW - 1)\n    )\n    else :\n        window = (\n            Window.partition_by(partition_col) \\\n            .order_by(order_col) \\\n            .rows_between(Window.CURRENT_ROW  - lag_param[lag], Window.CURRENT_ROW - 1 )\n        )\n    return df.with_column(col_name, F.avg(agg_col).over(window))\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e8a132c-83af-4d6b-bbd4-bfe1d6953eb0",
   "metadata": {
    "language": "python",
    "name": "create_columns_price_and_price_change"
   },
   "outputs": [],
   "source": "# Create Day of the Week Features Columns for Price & Price Change with varying lags \nlag_df = [\"HIST\",\"YEAR\",\"MONTH\"]\nagg_feat_columns = [ \"PRICE\",\"PRICE_CHANGE\"] \norder_columns = [\"YEAR\",\"MONTH\"]\npartition_columns = [\"MENU_ITEM_ID\",\"DAY_OF_WEEK\"]\nlag_param = { \"YEAR\" : 12, \"MONTH\" : 1 }\nnew_cols = []\n\nfor agg_col in agg_feat_columns :\n    col_name = agg_col\n    for lag in lag_df :\n        new_col_name = col_name + \"_\" + lag + \"_DOW\"\n        new_cols.append(new_col_name)\n        sales_agg_df = create_window_columns(sales_agg_df, new_col_name, agg_col, lag, \\\n                                             partition_columns, order_columns, lag_param)\n        ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c41d6ddf-5f39-4df2-9386-b96965a41701",
   "metadata": {
    "language": "python",
    "name": "create_average_features_columns_for_price_and_price_change"
   },
   "outputs": [],
   "source": "# Create Rolling Average Features Columns for Price & Price Change with varying lags \nlag_df = [\"HIST\",\"YEAR\",\"MONTH\"]\nagg_feat_columns = [ \"PRICE\",\"PRICE_CHANGE\"] \norder_columns = [\"YEAR\",\"MONTH\",\"DAY_OF_WEEK\"]\npartition_columns = ['MENU_ITEM_ID']\nlag_param = { \"YEAR\" : 84, \"MONTH\" : 7 }\n\nfor agg_col in agg_feat_columns :\n    col_name = agg_col\n    for lag in lag_df :\n        new_col_name = col_name + \"_\" + lag + \"_ROLL\"\n        new_cols.append(new_col_name)\n        sales_agg_df = create_window_columns(sales_agg_df, new_col_name, agg_col, lag, \\\n                                             partition_columns, order_columns, lag_param)\n",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell13"
   },
   "cell_type": "markdown",
   "source": [
    "### Impute Missing Values\n",
    "\n",
    "We check for columns with any missing values and replace null values by 0 using Snowpark built in function - ifnull."
   ],
   "id": "911d8ceb-34c2-4ae5-bc70-e5ec4818908f"
  },
  {
   "cell_type": "code",
   "id": "a44a7768-eb28-4165-9f38-7e0bd40c2111",
   "metadata": {
    "language": "python",
    "name": "check_for_null_columns"
   },
   "outputs": [],
   "source": "# Check for columns which have nulls\nsales_agg_df_pandas = sales_agg_df.toPandas()\nprint(sales_agg_df_pandas[sales_agg_df_pandas.columns[sales_agg_df_pandas \\\n                                                              .isnull().any()]].isnull().sum())\nnan_cols = [i for i in sales_agg_df_pandas.columns if sales_agg_df_pandas[i].isnull().any()]\n\n# Replace null values by 0 using Snowpark built in function - ifnull\nfor col in nan_cols:\n    sales_agg_df = sales_agg_df.with_column(col, F.call_builtin(\"ifnull\", F.col(col), 0))",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell14"
   },
   "cell_type": "markdown",
   "source": "### Final features post feature engineering",
   "id": "8d81f4b3-c671-4471-9432-17b08539fe4e"
  },
  {
   "cell_type": "code",
   "id": "aa26ce2f-495d-480b-84c4-a5959c1e0424",
   "metadata": {
    "language": "python",
    "name": "show_sales_agg_df_columns"
   },
   "outputs": [],
   "source": "sales_agg_df.columns",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell15"
   },
   "cell_type": "markdown",
   "source": [
    "### Save data to Snowflake table\n",
    "\n",
    "We split the prepped data into training & testing Snowpark dataframe. We will utilize data from last 2 months as testing data. "
   ],
   "id": "1b548d93-e8dd-4801-8da6-db21388a87cb"
  },
  {
   "cell_type": "code",
   "id": "efca1d4c-81f9-4713-9b26-0f76d9ecd858",
   "metadata": {
    "language": "python",
    "name": "define_training_and_testing_data"
   },
   "outputs": [],
   "source": "# Training data\ntrain_df = sales_agg_df.filter((F.col(\"YEAR\") < F.lit(max_year)) | \\\n                               ((F.col(\"YEAR\") ==  F.lit(max_year)) & \\\n                                (F.col(\"MONTH\") <  F.lit(max_month) - 1))) \n\n# Testing data\ntest_df = sales_agg_df.filter((F.col(\"YEAR\") == F.lit(max_year)) & \\\n                              (F.col(\"MONTH\") >= F.lit(max_month) - 1 ) & \\\n                              (F.col(\"MONTH\") < F.lit(new_month))\n                             )",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell16"
   },
   "cell_type": "markdown",
   "source": "We will save our training, test and all with full prepped datasets to the analytics schema in our Snowflake account.",
   "id": "589f42a4-180f-4a7f-8a06-0486524d06fb"
  },
  {
   "cell_type": "code",
   "id": "0d235bd1-8d02-4262-9c1b-1aefe1e17632",
   "metadata": {
    "language": "python",
    "name": "save_training_and_test_data"
   },
   "outputs": [],
   "source": "# Save training data\ntrain_df.write.mode(\"overwrite\").save_as_table(\n    \"demand_est_input_train\"\n)\n\n# Save test data\ntest_df.write.mode(\"overwrite\").save_as_table(\n    \"demand_est_input_test\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0970948-89dc-42c9-8584-106d0bf98ff6",
   "metadata": {
    "language": "python",
    "name": "save_full_data"
   },
   "outputs": [],
   "source": "# Save full data\nsales_agg_df.write.mode(\"overwrite\").save_as_table(\n    \"demand_est_input_full\"\n)",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell17"
   },
   "cell_type": "markdown",
   "source": [
    "# Model Training\n",
    "\n",
    "## Hyperparameter tuning using Snowflale ML modelling API \n",
    "\n",
    "We leverage Snowflake ML API to perform **distributed hyperparameter tuning** in order to train multiple models in parallel to learn to how menu-item demand is influenced by their prices . The `model.fit()` function creates a temporary stored procedure in the background enabling us to leverage Snowflake compute to train the models without moving the data outside the secure Snowflake environment.\n",
    "\n",
    "**NOTE** - Below cell may take upto 10 mins to complete."
   ],
   "id": "f1f7b613-6707-4c4a-8b4a-4c64831b8136"
  },
  {
   "cell_type": "code",
   "id": "be8aea13-be8a-46bb-8a61-10a058821671",
   "metadata": {
    "language": "python",
    "name": "get_and_fit_training_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "# %%time\n# Get training data\ntrain_df = session.table(\"demand_est_input_train\")\n\n# Use price columns for features \nprice_cols = [i for i in train_df.columns if \"PRICE\" in i]\n\n# Grid Search for XGBRegressor using Snowpark ML\ngrid_search = GridSearchCV(\n    estimator=XGBRegressor(),\n    param_grid={\n        \"n_estimators\":[100, 200, 300, 400, 500],\n        \"learning_rate\":[0.1, 0.2, 0.3, 0.4, 0.5],\n    },\n    cv=5,\n    n_jobs = -1,\n    scoring=\"neg_mean_absolute_percentage_error\",\n    input_cols=price_cols,\n    label_cols=\"TOTAL_QUANTITY_SOLD\",\n    output_cols=\"DEMAND_ESTIMATION\"\n)\n\n# Fit on training data\ngrid_search.fit(train_df)",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell18"
   },
   "cell_type": "markdown",
   "source": "We can look at the best parameters for model training based on the grid search by accessing best parameter attribute of the **fitted sklearn object**.",
   "id": "6995e379-4c82-4d50-a306-5727d8089109"
  },
  {
   "cell_type": "code",
   "id": "70882213-74f1-43ff-bae8-636893c50ff3",
   "metadata": {
    "language": "python",
    "name": "get_learning_rate_and_estimators",
    "collapsed": false
   },
   "outputs": [],
   "source": "best_params_ = grid_search.to_sklearn().best_params_\nbest_params_",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell19"
   },
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Model is evaluated is using different metrics from Snowpark ML Modeling -  mean_squared_error, mean_absolute_error, r2_score and tested on our prepped test data. We compare the predictions againts the actual historical sale."
   ],
   "id": "a386b6db-a7f2-4cfb-9bea-cd6d7dd35b63"
  },
  {
   "cell_type": "code",
   "id": "92205ddb-ce64-4767-9f79-e26562541123",
   "metadata": {
    "language": "python",
    "name": "get_test_data_and_model_metrics",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Test data\ntest_df = session.table(\"demand_est_input_test\")\n\n# Prediction\ndf_test_pred = grid_search.predict(test_df)\n\n# Model metrics\nmse = mean_squared_error(df=df_test_pred, y_true_col_names=\"TOTAL_QUANTITY_SOLD\", y_pred_col_names=\"DEMAND_ESTIMATION\")\nmae = mean_absolute_error(df=df_test_pred, y_true_col_names=\"TOTAL_QUANTITY_SOLD\", y_pred_col_names=\"DEMAND_ESTIMATION\")\nr2 = r2_score(df=df_test_pred, y_true_col_name=\"TOTAL_QUANTITY_SOLD\", y_pred_col_name=\"DEMAND_ESTIMATION\")\nprint(f'MSE: {mse}')\nprint(f'MAE: {mae}')\nprint(f'R2: {r2}')",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell20"
   },
   "cell_type": "markdown",
   "source": [
    "# Model Inference\n",
    "\n",
    "## Deploy model to Model Registry"
   ],
   "id": "af83ab6b-6dfb-4a0d-83c4-210afceda4ed"
  },
  {
   "cell_type": "code",
   "id": "9aca8ee4-89bf-4db9-87e6-5e0135167e34",
   "metadata": {
    "language": "python",
    "name": "get_latest_model"
   },
   "outputs": [],
   "source": "# Helper functions \n\n# Function to know if model with the specified name is logged on the given registry\ndef model_exists(reg, model_name):\n    models = reg.show_models()\n    if (models.empty) or (model_name not in models[\"name\"].to_list()) :\n        return False\n    return True\n\n# Function to get the latest version of a given model on the specified registry\ndef get_latest_version(reg, model_name):\n    models = reg.show_models()\n    if model_exists(reg, model_name):\n        return int(max(ast.literal_eval(models.loc[models[\"name\"] == model_name, \"versions\"].values[0]))[1:])\n    return 0",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell21"
   },
   "cell_type": "markdown",
   "source": [
    "### Opening Snowpark Model Registry\n",
    "Before we can deploy our model, we must open the registry. Opening the registry returns a reference to it, which we use to add new models and obtain references to existing models. Below, we open a registry with our default database and schema. "
   ],
   "id": "1ed02812-ae02-464d-aa27-f94df8cc259e"
  },
  {
   "cell_type": "code",
   "id": "722ea185-faf2-43d8-a585-d38d780a7d12",
   "metadata": {
    "language": "python",
    "name": "show_current_models"
   },
   "outputs": [],
   "source": "# Use the session default database and schema for model registry\nreg = Registry(session=session) \n\n# Models logged on the registry\nmodel_df = reg.show_models()\nmodel_df",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell22"
   },
   "cell_type": "markdown",
   "source": "With registry opened, we can see if there are an pre-existing models using `registry.show_models()` function which returns a dataframe with details of the models  available on the registry. Currently, we have no models deployed on the registry hence the dataframe would be empty.",
   "id": "4197abc4-330d-4752-a33b-76954f07f1dd"
  },
  {
   "metadata": {
    "name": "cell23"
   },
   "cell_type": "markdown",
   "source": [
    "### Log model to the registry\n",
    "\n",
    "Log a model by calling the `registry.log_model()` function. This function does the following:\n",
    "- Serializes the model, a Python object, and creates a Snowflake model object from it.\n",
    "- Adds metadata, such as a description, to the model as specified in the log_model call."
   ],
   "id": "c259f156-8485-4ef5-a8ca-0a5fddde25a1"
  },
  {
   "cell_type": "code",
   "id": "2b70b180-b083-4d17-81d8-8ac9e69b46d0",
   "metadata": {
    "language": "python",
    "name": "log_model"
   },
   "outputs": [],
   "source": "# Provide name for the model \nmodel_name = \"DEMAND_ESTIMATION_MODEL\"\n\n# Log the model\nmv = reg.log_model(model=grid_search, \\\n    model_name= model_name, \\\n    version_name= \"V\"+str(get_latest_version(reg, model_name) + 1), \\\n    metrics={\"mean_squared_error\":mse, \"mean_absolute_error\":mae, \"r2_score\":r2}, \n    comment=\"Demand estimation ML model based on price features\")",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell24"
   },
   "cell_type": "markdown",
   "source": "Below, we can look at our newly logged model.  ",
   "id": "1989a9ef-7ed9-4f4a-8a8c-afb93aac7b37"
  },
  {
   "cell_type": "code",
   "id": "eb1df4b6-a599-4252-8b15-00d7dad3dacb",
   "metadata": {
    "language": "python",
    "name": "show_logged_models"
   },
   "outputs": [],
   "source": "# Models logged on the registry\nmodel_df = reg.show_models()\nmodel_df",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "Each model may have any number of versions. To log additional versions of the model, we can call log_model again with the same model_name but a different version_name.\n\nWe can also explore all the versions of a model by utilizing `model.show_versions()` function. Currently, we have only one version logged for our model. In an event we were to retrain the model, we can log it under the same model name with a new version.",
   "id": "aa77bb79-6761-4dce-b859-90fbd81a17df"
  },
  {
   "cell_type": "code",
   "id": "d11213bb-af87-4b19-a86d-bb574fb3a421",
   "metadata": {
    "language": "python",
    "name": "show_model_version"
   },
   "outputs": [],
   "source": "# Versions available for a given model on the registry\nmodel_versions = reg.get_model(model_name).show_versions()\nmodel_versions",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "### Accessing model prediction\nWith our model now logged onto the registry, we can access it's associated functions that can be executed to perform inference or other model operations.\n\nTo call a function for our model, we use `mv.run` specifying the name of the function to be called and passing a DataFrame containing the inference data. Below, we call the \"predict\" function of our model to get predictions and infer on our test data. ",
   "id": "a4430f28-1190-416e-9df0-f1e0daa79381"
  },
  {
   "cell_type": "code",
   "id": "3b5ed0be-4c20-4659-a145-38d486879b24",
   "metadata": {
    "language": "python",
    "name": "show_predictions"
   },
   "outputs": [],
   "source": "prediction = mv.run(test_df.select(price_cols), function_name=\"predict\")\nprediction.show()",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "# Model Utilization\n\n### Create a Stored Procedure which functions as Price Recommender: \n**Step 1. Create a Function for get day of the week price recommendations for all menu-items for a given month**",
   "id": "5da62a04-8590-42bc-85d0-ebaabe2ebcfe"
  },
  {
   "metadata": {
    "name": "cell28"
   },
   "cell_type": "markdown",
   "source": "Let's create Snowpark Stored procedure to utilize the deployed model to predict demand for all possible prices for each menu-item. The possible prices for a given menu-item range from 50% discount of it's base sale price to 20% mark-up on it's base sale price. Once we get the predicted demand at all possible price for each menu-item, we calculate the profit on that demand by accounting for cost of goods, item profit & basket profit. The recommded price would then be price which estimates the maximum profit. Finally, these recommendations are stored to a Snowflake table.",
   "id": "7bf5fed5-6a3c-468e-8b01-affac6ba5685"
  },
  {
   "cell_type": "code",
   "id": "06bc3cb8-1fec-4e52-99f6-771426374982",
   "metadata": {
    "language": "python",
    "name": "get_recommendations_function_definition"
   },
   "outputs": [],
   "source": "def get_recommendations(\n    session: Session,\n    month: int,\n    year: int,\n    price_rec_table: str,\n    interval: int, \n    model_name: str\n) -> T.Variant :\n        from snowflake.ml.registry.registry import Registry\n        \n        price_cols = ['PRICE', 'PRICE_CHANGE', 'BASE_PRICE', 'PRICE_HIST_DOW', 'PRICE_YEAR_DOW', 'PRICE_MONTH_DOW', 'PRICE_CHANGE_HIST_DOW', \\\n                        'PRICE_CHANGE_YEAR_DOW', 'PRICE_CHANGE_MONTH_DOW', 'PRICE_HIST_ROLL', 'PRICE_YEAR_ROLL', 'PRICE_MONTH_ROLL', 'PRICE_CHANGE_HIST_ROLL', \\\n                        'PRICE_CHANGE_YEAR_ROLL', 'PRICE_CHANGE_MONTH_ROLL']\n                        \n        # Create dataframe with feature columns using prepped table filtered to our input\n        item_df = session.table(\"demand_est_input_full\") \\\n                        .filter((F.col(\"YEAR\") == F.lit(year)) \\\n                                & (F.col(\"MONTH\") == F.lit(month))) \\\n                        .drop([\"PRICE\",\"PRICE_CHANGE\"])\n\n\n        # Possible price dataframe\n        discount_df = session.create_dataframe(list(np.arange(50, -21, -interval)), \\\n                                                schema=T.StructType([T.StructField(\"DISCOUNT\", T.IntegerType())]),)\n\n        # Cross-join with item_df to get record to infer on\n        item_df = item_df.cross_join(discount_df) \\\n                        .with_column(\"PRICE\", F.round((1 - F.col(\"DISCOUNT\") * 0.01) * F.col(\"BASE_PRICE\"), 2)) \\\n                        .with_column(\"PRICE_CHANGE\", F.col(\"PRICE\") - F.col(\"BASE_PRICE\"))\n\n        # Get demand estimator model from registry\n        reg = Registry(session=session) \n        demand_estimator = reg.get_model(model_name).default\n \n        for col in price_cols :\n                item_df = item_df.withColumn(col+\"_NEW\",F.col(col).cast(T.DoubleType())).drop(col).rename(col+\"_NEW\",col)\n           \n        # Estimate demand & corresponding item profit based on varying price using Demand Estimator model predict function\n        item_df = demand_estimator.run(item_df, function_name=\"predict\") \\\n                .with_column(\"ITEM_PROFIT\", \\\n                                        (F.col(\"DEMAND_ESTIMATION\") * F.col(\"PRICE\")) - \\\n                                        (F.col(\"DEMAND_ESTIMATION\") * F.round(F.col(\"COST_OF_GOODS_USD\"), 2)),)\n                                        \n        # Get avg basket profit for previous month for the given item\n        basket_profit = (session.table(\"order_item_cost_agg_v\") \\\n                        .filter((F.col(\"YEAR\") == F.lit(year)) \\\n                                & (F.col(\"MONTH\") == F.lit(month))) \\\n                                .select(\"MENU_ITEM_ID\", \"PREV_AVG_PROFIT_WO_ITEM\")).cache_result()\n\n        # Recommend with highest profit\n        window = Window.partition_by([\"MENU_ITEM_ID\", \"DAY_OF_WEEK\"]).order_by(F.col(\"TOTAL_PROFIT\").desc())\n        item_df = (item_df.join(basket_profit, \"MENU_ITEM_ID\") \\\n                .with_column(\"BASKET_PROFIT\", F.col(\"DEMAND_ESTIMATION\") * F.col(\"PREV_AVG_PROFIT_WO_ITEM\"),) \\\n                .with_column(\"TOTAL_PROFIT\", F.col(\"BASKET_PROFIT\") + F.col(\"ITEM_PROFIT\")) \\\n                .order_by(F.col(\"PRICE\").asc()) \\\n                .with_column(\"MAX_PROFIT_IND\", F.row_number().over(window))) \\\n        .filter(F.col(\"MAX_PROFIT_IND\") == 1)\n\n        # Save recommendation to Snowflake\n        item_df.select(\"TRUCK_BRAND_NAME\", \"MONTH\", \"YEAR\", \"DAY_OF_WEEK\",\"MENU_ITEM_ID\", \\\n                                F.round(F.col(\"COST_OF_GOODS_USD\"),2).alias(\"COST_OF_GOODS_USD\"), \\\n                                \"BASE_PRICE\", \"PRICE\",\"DEMAND_ESTIMATION\", \\\n                                F.round(F.col(\"ITEM_PROFIT\"),2).alias(\"ITEM_PROFIT\"), \\\n                                F.round(F.col(\"BASKET_PROFIT\"),2).alias(\"BASKET_PROFIT\"), \\\n                                F.col(\"TOTAL_PROFIT\"),\n                                ).write.mode(\"overwrite\") \\\n        .save_as_table(price_rec_table)\n        \n        return \"Recommendations complete\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8179ef6a-70eb-4d92-bd03-c3c16c138c00",
   "metadata": {
    "language": "python",
    "name": "create_ude_and_stored_procedure_stage"
   },
   "outputs": [],
   "source": "# Create stage to host UDF & Stored Procedures\nsession.sql(\"CREATE STAGE IF NOT EXISTS PO_STAGE\").collect()",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell29",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "**Step 2. Register the Function on Snowflake** \n\nTo register the function on Snowflake as a stored procedure, specify what python packages are required in the function. ",
   "id": "53452b12-2aa8-4599-bc12-d05f5499b803"
  },
  {
   "cell_type": "code",
   "id": "a3ad60cf-90c9-4473-b834-3132186577d1",
   "metadata": {
    "language": "python",
    "name": "registering_stored_procedure"
   },
   "outputs": [],
   "source": "# Register stored procedure\nget_recommendations_sp = session.sproc.register(\n    func=get_recommendations,\n    name=\"sproc_get_recommendations\",\n    is_permanent=True,\n    replace=True,\n    stage_location=\"@PO_STAGE\",\n    packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python==1.4.0\"]\n)",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "# Data Driven Insights\n\n**Peking Truck** food-truck brand has seen **lower than average Wednesday sales** for **Wonton Soup** for the past few months. The Brand Manager proposed introducing a Wednesday Wonton promotion to drive up the sales. In order to get the best promotion price, Tasty Bytes utilized deployed price recommender stored procedure to drive up sales.\n\nWe will start off by **visualizing Wonton Wednesday Sales & Price** for last 6 months to understand if there might be a relation between the two.",
   "id": "47479bb5-7fd3-42b8-80e4-2b3552d64628"
  },
  {
   "cell_type": "code",
   "id": "53f9e2cd-61fe-4ebe-9329-f337aa209433",
   "metadata": {
    "language": "python",
    "name": "wonton_wednesday_sales_last_6_months"
   },
   "outputs": [],
   "source": "## Wonton Wednesday Sales for last 6 months\nwonton_agg_df = sales_agg_df.filter(\n                   (((F.col(\"YEAR\") == F.lit(2022)) & \\\n                    (F.col(\"MONTH\").in_(F.lit(10), F.lit(11), F.lit(12)))) |\n                    ((F.col(\"YEAR\") == F.lit(2022)) & (F.col(\"MONTH\").in_(F.lit(1),F.lit(2),F.lit(3))))\n                   ) & \\\n                   (F.col(\"DAY_OF_WEEK\") == F.lit(3)) & \\\n                   (F.col(\"MENU_ITEM_ID\") == F.lit(133))).order_by([F.col(\"YEAR\"),F.col(\"MONTH\")]) \\\n                .select(\n                    F.col(\"TOTAL_QUANTITY_SOLD\"), \\\n                    F.col(\"PRICE_CHANGE\"), \\\n                    F.col(\"PRICE\"), \\\n                    F.col(\"BASE_PRICE\"), \\\n                    F.concat(F.col(\"YEAR\"),F.lit('-'),F.col(\"MONTH\")).alias(\"MONTH\")) \\\n                 .toPandas()\n\navg_wonton_sale = sales_agg_df.filter(\n                   (F.col(\"YEAR\") == F.lit(2022)) & \\\n                   (F.col(\"DAY_OF_WEEK\") == F.lit(3)) & \\\n                   (F.col(\"MENU_ITEM_ID\") == F.lit(133))).order_by([F.col(\"YEAR\"),F.col(\"MONTH\")]) \\\n                    .agg(F.avg(\"TOTAL_QUANTITY_SOLD\").alias(\"TOTAL_QUANTITY_SOLD\")).collect()[0][0]\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7eff46f8-36e4-4527-9517-d4f44ee2347b",
   "metadata": {
    "language": "python",
    "name": "plot_average_monthly_wonton_soup_sales"
   },
   "outputs": [],
   "source": "# Plot Average Monthly Wednesday Sales for Wonton Soup\nplt.rcParams[\"figure.figsize\"] = [6.00, 3.50]\nlp = sns.lineplot(data=wonton_agg_df, x=\"MONTH\", y=\"TOTAL_QUANTITY_SOLD\")\nplt.axhline(y=avg_wonton_sale, color=\"r\", linestyle=\"-\", label=\"Average Wednesday Wonton Sales\")\nplt.title(\"Wednesday Sales for Wonton Soup\")\nplt.legend( loc='lower right')\nplt.xlabel(\"Month\")\nplt.ylabel(\"Quatity Sold\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d8cdea0-c3bd-4284-9bdc-b7f17e332a7f",
   "metadata": {
    "language": "python",
    "name": "plot_average_monthly_wonton_soup_prices"
   },
   "outputs": [],
   "source": "#Plot Average Monthly Wednesday Prices for Wonton Soup\nprice = sns.lineplot(data=wonton_agg_df, x=\"MONTH\", y=\"PRICE\")\nplt.title(\"Wednesday Prices for Wonton Soup\")\nplt.xticks(range(len(wonton_agg_df[\"MONTH\"])), wonton_agg_df[\"MONTH\"], rotation='vertical')\nplt.xlabel(\"Month\")\nplt.ylabel(\"Price\")\nplt.show()",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell31",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "We can see the drop in the sales closely follows an incresase in price.\n\n**Scale up our Snowflake compute**\n\nWe will dynamically change to a larger compute warehouse without restarting our Python kernel or needing to redefine variables. Here we use a 2XL warehouse before calling price recommender stored procedure.",
   "id": "c6fe55d8-71fd-48c8-a1c9-90fb6dfeef9d"
  },
  {
   "cell_type": "code",
   "id": "15b2c0b5-0c80-446a-9c04-df5994339bb6",
   "metadata": {
    "language": "python",
    "name": "scale_up_compute",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Scale up the compute\nsession.sql(f\"ALTER WAREHOUSE TB_PO_DS_WH SET WAREHOUSE_SIZE = X2LARGE\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73b6f0ec-afc2-4abe-85a9-a4b8ac204f9f",
   "metadata": {
    "language": "python",
    "name": "show_model_version_on_registry"
   },
   "outputs": [],
   "source": "# Versions available for a given model on the registry\nmodel_versions = reg.get_model(model_name).show_versions()\nmodel_versions",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "Next up, **we call the price recommender to get best promotion price** for April 2023. The stored procedure writes the recommendation to Snowflake table - promotion_recommendations.\n\nNOTE - The cell below takes about 30 secs to run.",
   "id": "097027a8-e96a-463a-9d37-5570287abc1b"
  },
  {
   "cell_type": "code",
   "id": "018a9c50-c55c-4fc8-bdc8-babddef987b0",
   "metadata": {
    "language": "python",
    "name": "call_stored_proc_on_promotion_recommendations",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Input\nmonth = 4 \nyear = 2022\nprice_rec_table = \"promotion_recommendations\" \n\n# Call stored procedure\nprint(get_recommendations_sp(session, month, year,price_rec_table, 10, \"DEMAND_ESTIMATION_MODEL\"))\n\n",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell33"
   },
   "cell_type": "markdown",
   "source": [
    "**Scale Down Snowflake Compute**\n",
    "\n",
    "Once complete, we will scale our compute warehouse back down. The larger warehouse size is not required for the remaining tasks."
   ],
   "id": "40a47fd4-ad70-4068-a67d-b49773317815"
  },
  {
   "cell_type": "code",
   "id": "b5960bc4-4f3b-43a7-9fb1-e62692728117",
   "metadata": {
    "language": "python",
    "name": "scale_down_compute",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Scale down the compute\nsession.sql(f\"ALTER WAREHOUSE TB_PO_DS_WH SET WAREHOUSE_SIZE = XSMALL\").collect()",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell34"
   },
   "cell_type": "markdown",
   "source": "Let's look at the recommended price for Wonton Soup. We see a **recommendation of a lower price compared to its base sale price**. We are also provided with the estimated demand & profit at the recommended price. ",
   "id": "d9efd3d3-d6d4-477f-baf0-9f5384abd22b"
  },
  {
   "cell_type": "code",
   "id": "a6cee898-4e5b-4b81-b46c-028f985f25bd",
   "metadata": {
    "language": "python",
    "name": "wonton_recommendation",
    "collapsed": false
   },
   "outputs": [],
   "source": "wonton_wed_recommendations = session.table(price_rec_table) \\\n                            .filter((F.col(\"DAY_OF_WEEK\") == F.lit(3)) & \\\n                                    (F.col(\"MENU_ITEM_ID\") == F.lit(133))) \\\n                            .with_column_renamed(F.col(\"PRICE\"), \"RECOMMENDED_PRICE\").toPandas()\nwonton_wed_recommendations",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "Peking Truck food-trucks apply the recommended discounted price for Wonton on Wednesdays for April 2022.",
   "id": "0df852e7-5550-4a60-a21e-3662740225a0"
  },
  {
   "cell_type": "code",
   "id": "8e39f5c3-0927-4078-bce9-60c821d9c3e3",
   "metadata": {
    "language": "python",
    "name": "plot_wonton_sales_from_feb_to_april_2022",
    "collapsed": false
   },
   "outputs": [],
   "source": "## Wonton Wednesday Sales for Feb - Apr 2022\nwonton_agg_df = sales_agg_df.filter(\n                                   (F.col(\"YEAR\") == F.lit(2022)) & \\\n                                   (F.col(\"MONTH\").in_(F.lit(1), F.lit(2), F.lit(3), F.lit(4))) & \\\n                                   (F.col(\"DAY_OF_WEEK\") == F.lit(3)) & \\\n                                   (F.col(\"MENU_ITEM_ID\") == F.lit(133))) \\\n                            .select(\n                                    F.col(\"TOTAL_QUANTITY_SOLD\"),  F.col(\"PRICE\"), \\\n                                    F.concat(F.col(\"YEAR\"),F.lit('-'),F.col(\"MONTH\")).alias(\"MONTH\")) \\\n                            .order_by(F.col(\"MONTH\")).toPandas()\n\n# Plot Average Monthly Wednesday Sales for Wonton Soup\nplt.rcParams[\"figure.figsize\"] = [6.00, 3.50]\nlp = sns.lineplot(data=wonton_agg_df, x=\"MONTH\", y=\"TOTAL_QUANTITY_SOLD\")\nplt.axhline(y=avg_wonton_sale, color=\"r\", linestyle=\"-\", label=\"Average Wednesday Wonton Sales\")\nplt.title(\"Wednesday Sales for Wonton Soup\")\nplt.legend( loc='lower right')\nplt.xlabel(\"Month\")\nplt.ylabel(\"Quatity Sold\")\nplt.show()",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "We can see a spike in the sales post the promotion being introduced for Peking Trucks.\n\n**Output for Price Optimization SiS App**\nTasty Bytes also utilizes the price recommender to get optimum prices for all menu-items apart from special promotional prices. These price recommendations are leveraged downstream by the SiS App to inform food-truck brand pricing strategy. \n\nNOTE - The cell below takes about 30 secs to run.",
   "id": "d12d0f07-21d0-47f5-a6c7-9a5621fcb5e4"
  },
  {
   "cell_type": "code",
   "id": "0f6b10af-fa57-4d27-8454-e8697923bb2d",
   "metadata": {
    "language": "python",
    "name": "call_stored_proc_on_price_recommendation",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Input\nmonth = 4 \nyear = 2022\nprice_rec_table = \"price_recommendations\" \n\n\n# Scale up the compute\nsession.sql(f\"ALTER WAREHOUSE TB_PO_DS_WH SET WAREHOUSE_SIZE = X2LARGE\").collect()\n\n# Call stored procedure\nprint(get_recommendations_sp(session, month, year,price_rec_table, 1, \"DEMAND_ESTIMATION_MODEL\"))\n\n# Scale down the compute\nsession.sql(f\"ALTER WAREHOUSE TB_PO_DS_WH SET WAREHOUSE_SIZE = XSMALL\").collect()",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell37",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "# Summary\n**- Securely connected to Snowflake via Snowpark**\n\n**- Performed Feature Engineering by leveraging Snowpark DataFrame to query & transform Snowflake data**\n\n**- Save prepped data to Snowflake table**\n\n**- Employed Snowpark Stored Procedure to train ML model**\n\n**- Deployed model to perform inference using Snowpark User Defined Functions**\n\n**- Utilizing deployed model using Snowpark Stored Procedure**\n",
   "id": "2c319de9-49c5-4327-b9e2-02866741f087"
  },
  {
   "metadata": {
    "name": "cell38",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "#### Demo: frostbyte Tasty Bytes\nVersion: v1\n\nVignette: Price Optimization\n\nScript: tasty_bytes_price_optimization\n\nCreate Date:    2023-06-01\n\nAuthor:         Shriya Rai\n\n#### Description:\n**Tasty Bytes is one of the largest food truck networks** in the world with localized menu options spread across  **15 food truck brands** globally. **Tasty Bytes is aiming to achieve 25% YoY sales growth over 5 years.**\n\n**Price optimization** enables Tasty Bytes to achieve this goal by **determining the right prices** for their menu items to **maximize profitability** while **maintaining customer satisfaction**.\n\nAs Tasty Bytes Data Scientists, **we will implement price optimization for their diversified food-truck brands to inform their pricing and promotions**.\n\nIn this notebook, we will train & deploy an ML model which leverages historical menu-item sale data to understand how menu-item demand changes with varying price. By utilizing this trained model, we would recommend the optimal day of week prices for all menu-items for the upcoming month to our food-truck brands.\n\n#### Summary of Changes:\n\n| Date(yyyy-mm-dd) | Author         | Comments                      |\n| :---             | :---           | :---                                                  |\n| 2023-06-01       | Shriya Rai | Initial Release |\n| 2024-03-07       | Shriya Rai | Update with Snowpark ML |\n",
   "id": "c3e1187d-6922-4e6f-acc1-254e7c282db1"
  },
  {
   "cell_type": "code",
   "id": "fd319031-3c70-4afd-864c-da54165d243c",
   "metadata": {
    "language": "sql",
    "name": "streamlit_app_setup",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/***************************************************************************************************\n",
    "  _______           _            ____          _\n",
    " |__   __|         | |          |  _ \\        | |\n",
    "    | |  __ _  ___ | |_  _   _  | |_) | _   _ | |_  ___  ___\n",
    "    | | / _` |/ __|| __|| | | | |  _ < | | | || __|/ _ \\/ __|\n",
    "    | || (_| |\\__ \\| |_ | |_| | | |_) || |_| || |_|  __/\\__ \\\n",
    "    |_| \\__,_||___/ \\__| \\__, | |____/  \\__, | \\__|\\___||___/\n",
    "                          __/ |          __/ |\n",
    "                         |___/          |___/\n",
    "Demo:         Tasty Bytes - Price Optimization SiS\n",
    "Version:      v1\n",
    "Vignette:     2 - SiS with Snowpark        \n",
    "Create Date:  2023-06-08\n",
    "Author:       Marie Coolsaet\n",
    "Copyright(c): 2023 Snowflake Inc. All rights reserved.\n",
    "****************************************************************************************************\n",
    "Description: \n",
    "   Create tables used in SiS Streamlit App for Setting Monthly Pricing\n",
    "****************************************************************************************************\n",
    "SUMMARY OF CHANGES\n",
    "Date(yyyy-mm-dd)    Author              Comments\n",
    "------------------- ------------------- ------------------------------------------------------------\n",
    "2023-06-08        Marie Coolsaet      Initial Release\n",
    "2024-03-07        Shriya Rai          Update with Snowpark ML \n",
    "***************************************************************************************************/\n",
    "\n",
    "/*----------------------------------------------------------------------------------\n",
    "Instructions: Run all of this script to create the required tables and roles for the SiS app.\n",
    "\n",
    "Note: In order for these scripts to run you will need to have run the notebook in\n",
    "tasty_bytes_price_optimization.ipynb in 1 - Machine Learning with Snowpark.\n",
    "\n",
    " ----------------------------------------------------------------------------------*/\n",
    "\n",
    "USE ROLE tb_po_data_scientist;\n",
    "USE WAREHOUSE tb_po_ds_wh;\n",
    "\n",
    "ALTER warehouse tb_po_ds_wh SET warehouse_size='large';\n",
    "\n",
    "-- create the table that the app will write back to\n",
    "CREATE OR REPLACE TABLE tb_po_prod.analytics.pricing_final (\n",
    "    brand VARCHAR(16777216),\n",
    "    item VARCHAR(16777216),\n",
    "    day_of_week VARCHAR(16777216),\n",
    "    new_price FLOAT,\n",
    "    current_price FLOAT,\n",
    "    recommended_price FLOAT,\n",
    "    profit_lift FLOAT,\n",
    "    comment VARCHAR(16777216),\n",
    "    timestamp TIMESTAMP_NTZ(9)\n",
    ");\n",
    "\n",
    "-- create the table with required pricing information for the app\n",
    "CREATE OR REPLACE TABLE tb_po_prod.analytics.pricing_detail AS\n",
    "SELECT \n",
    "    a.truck_brand_name AS brand,\n",
    "    a.menu_item_name AS item,\n",
    "    case \n",
    "        when a.day_of_week = 0 then '7 - Sunday'\n",
    "        when a.day_of_week = 1 then '1 - Monday'\n",
    "        when a.day_of_week = 2 then '2 - Tuesday'\n",
    "        when a.day_of_week = 3 then '3 - Wednesday'\n",
    "        when a.day_of_week = 4 then '4 - Thursday'\n",
    "        when a.day_of_week = 5 then '5 - Friday'\n",
    "        else '6 - Saturday'\n",
    "    end as day_of_week,\n",
    "    round(b.price::FLOAT,2) AS current_price,\n",
    "    round(a.price::FLOAT,2) AS recommended_price,\n",
    "    tb_po_prod.analytics.DEMAND_ESTIMATION_MODEL!PREDICT(\n",
    "        current_price,\n",
    "        current_price - c.base_price,\n",
    "        c.base_price,\n",
    "        c.price_hist_dow,\n",
    "        c.price_year_dow,\n",
    "        c.price_month_dow,\n",
    "        c.price_change_hist_dow,\n",
    "        c.price_change_year_dow,\n",
    "        c.price_change_month_dow,\n",
    "        c.price_hist_roll,\n",
    "        c.price_year_roll,\n",
    "        c.price_month_roll,\n",
    "        c.price_change_hist_roll,\n",
    "        c.price_change_year_roll,\n",
    "        c.price_change_month_roll):DEMAND_ESTIMATION::INT AS current_price_demand,\n",
    "    tb_po_prod.analytics.DEMAND_ESTIMATION_MODEL!PREDICT(\n",
    "        recommended_price,\n",
    "        recommended_price - c.base_price,\n",
    "        c.base_price,\n",
    "        c.price_hist_dow,\n",
    "        c.price_year_dow,\n",
    "        c.price_month_dow,\n",
    "        c.price_change_hist_dow,\n",
    "        c.price_change_year_dow,\n",
    "        c.price_change_month_dow,\n",
    "        c.price_hist_roll,\n",
    "        c.price_year_roll,\n",
    "        c.price_month_roll,\n",
    "        c.price_change_hist_roll,\n",
    "        c.price_change_year_roll,\n",
    "        c.price_change_month_roll):DEMAND_ESTIMATION::INT AS recommended_price_demand,\n",
    "    round(((recommended_price_demand\n",
    "        * (d.prev_avg_profit_wo_item \n",
    "            + recommended_price \n",
    "            - round(a.cost_of_goods_usd,2))) \n",
    "            - (current_price_demand\n",
    "        * (d.prev_avg_profit_wo_item \n",
    "            + current_price \n",
    "            - round(a.cost_of_goods_usd,2))))\n",
    "            ,0) AS profit_lift,\n",
    "    c.base_price,\n",
    "    c.price_hist_dow,\n",
    "    c.price_year_dow,\n",
    "    c.price_month_dow,\n",
    "    c.price_change_hist_dow,\n",
    "    c.price_change_year_dow,\n",
    "    c.price_change_month_dow,\n",
    "    c.price_hist_roll,\n",
    "    c.price_year_roll,\n",
    "    c.price_month_roll,\n",
    "    c.price_change_hist_roll,\n",
    "    c.price_change_year_roll,\n",
    "    c.price_change_month_roll,\n",
    "    d.prev_avg_profit_wo_item AS average_basket_profit,\n",
    "    round(a.cost_of_goods_usd,2) AS item_cost,\n",
    "    recommended_price_demand * (average_basket_profit\n",
    "            + recommended_price \n",
    "            - item_cost) AS recommended_price_profit,\n",
    "    current_price_demand * (average_basket_profit\n",
    "            + current_price\n",
    "            - item_cost) AS current_price_profit\n",
    "FROM (\n",
    "SELECT p.*,m.menu_item_name FROM tb_po_prod.analytics.price_recommendations p \n",
    "left join tb_po_prod.raw_pos.menu m on p.menu_item_id=m.menu_item_id) a\n",
    "LEFT JOIN (SELECT * FROM tb_po_prod.analytics.demand_est_input_full WHERE (month = 3) AND (year=2022)) b\n",
    "ON  a.day_of_week = b.day_of_week AND a.menu_item_id = b.menu_item_id\n",
    "LEFT JOIN (SELECT * FROM tb_po_prod.analytics.demand_est_input_full WHERE (month = 4) AND (year=2022)) c\n",
    "ON a.day_of_week = c.day_of_week AND a.menu_item_id = c.menu_item_id\n",
    "LEFT JOIN (SELECT * FROM tb_po_prod.analytics.order_item_cost_agg_v WHERE (month = 4) AND (year=2022)) d\n",
    "ON a.menu_item_id = d.menu_item_id\n",
    "ORDER BY brand, item, day_of_week;\n",
    "\n",
    "\n",
    "-- create pricing table to be displayed in the app\n",
    "CREATE OR REPLACE TABLE tb_po_prod.analytics.pricing \n",
    "AS SELECT\n",
    "    brand,\n",
    "    item,\n",
    "    day_of_week,\n",
    "    current_price AS new_price,\n",
    "    current_price,\n",
    "    recommended_price,\n",
    "profit_lift\n",
    "FROM\n",
    "    tb_po_prod.analytics.pricing_detail;"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell39"
   },
   "cell_type": "markdown",
   "source": [
    "# Price Recommendations\n",
    "Automated pipelines update ML-driven price recommendations for menu items by day-of-week on a monthly basis. Brand managers are responsible for adjusting pricing for menu items under their food truck brand. We will look at a spreadsheet provided by the Guac n' Roll brand manager for the upcoming month. We'll use the model to see the profit lift from the changes in pricing compared to last month's pricing."
   ],
   "id": "07150d12-6d02-4738-8dcd-150d4b7bb149"
  },
  {
   "metadata": {
    "name": "cell40",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "## View Price Recommendations\nRecommendations are for the upcoming month by day of the week for each menu item. By default, the new price is set to the current price unless overridden by the brand manager responsible for pricing of that specific food truck brand.\n\n**Value**: Easy deployment of ML models to provide updated insight.",
   "id": "916208b8-951a-4a30-acd7-c204b33e35b7"
  },
  {
   "cell_type": "code",
   "id": "3a59a045-c19e-402c-bd0f-f628df09c1a8",
   "metadata": {
    "language": "python",
    "name": "show_pricing_table",
    "collapsed": false
   },
   "outputs": [],
   "source": "brand = \"Guac n' Roll\"\nitem = \"Chicken Burrito\"\nsession.table(\"pricing\").filter((F.col(\"brand\")==brand) & (F.col(\"item\")==item)).show()",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "## Import New Pricing\nCurrently brand managers submit pricing by spreadsheet. The brand manager for Guac n' Roll provided their pricing so that it could be demonstrated how the model could be used to provide insight on demand and profit changes at different prices.\n\n**Value**: Snowflake provides an accessible, single version of the truth for data.",
   "id": "660ca071-eafd-433d-8695-992f207113ec"
  },
  {
   "cell_type": "code",
   "id": "794461b4-5b4c-4cfe-863b-66fa1df2ce2f",
   "metadata": {
    "language": "python",
    "name": "load_excel_file",
    "collapsed": false
   },
   "outputs": [],
   "source": "stage_name = 'TB_PO_PROD.PUBLIC.EXCEL_S3'\nfile_name = 'pricing_guac_roll_04_2023.xlsx'\n\nfile_path = session.file.get_stream(f'@{stage_name}/{file_name}')\nset_prices = pd.read_excel(file_path)\nset_prices = set_prices[set_prices[\"ITEM\"]==item]\nset_prices",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell42",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "## Forecast Demand of New Prices\nHere, we call the model that has been deployed to a user-defined function in Snowflake to predict the demand for the new prices that were set in the spreadsheet by the Guac n' Roll brand manager.\n\n**Value:** Access deployed models and apply them adhoc to new data.",
   "id": "6100d655-7b60-4a0d-a382-ea6d883b3273"
  },
  {
   "cell_type": "code",
   "id": "3ed4645d-2cf0-4bed-ad87-41d174bf688c",
   "metadata": {
    "language": "python",
    "name": "get_demand_estimator_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create Snowpark DataFrame\nset_prices = session.create_dataframe(set_prices)\n\n# Define model input features\nfeature_cols = [\n    \"price\",\n    \"price_change\",\n    \"base_price\",\n    \"price_hist_dow\",\n    \"price_year_dow\",\n    \"price_month_dow\",\n    \"price_change_hist_dow\",\n    \"price_change_year_dow\",\n    \"price_change_month_dow\",\n    \"price_hist_roll\",\n    \"price_year_roll\",\n    \"price_month_roll\",\n    \"price_change_hist_roll\",\n    \"price_change_year_roll\",\n    \"price_change_month_roll\",\n]\n\n# Get demand estimation\ndf_demand = set_prices.join(\n    session.table(\"pricing_detail\"), [\"brand\", \"item\", \"day_of_week\"]\n).withColumn(\"price\",F.col(\"new_price\")).withColumn(\"price_change\",F.col(\"PRICE\")- F.col(\"base_price\"))\n\n# Get demand estimator model from registry\nreg = Registry(session=session) \ndemand_estimator = reg.get_model(\"DEMAND_ESTIMATION_MODEL\").default\n\nfor col in feature_cols :\n        df_demand = df_demand.withColumn(col+\"_NEW\",F.col(col).cast(T.DoubleType())).drop(col).rename(col+\"_NEW\",col)\n    \ndf_demand = demand_estimator.run(df_demand, function_name=\"predict\")\\\n    .select(\n    \"day_of_week\",\n    \"current_price_demand\",\n    \"new_price\",\n    \"item_cost\",\n    \"average_basket_profit\",\n    \"current_price_profit\",\n    F.col(\"demand_estimation\").alias(\"new_price_demand\"))",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell43"
   },
   "cell_type": "markdown",
   "source": "### Visualize Predicted Demand of New vs. Current  Prices",
   "id": "5a02f5db-251a-473e-89f1-a56db1344e67"
  },
  {
   "cell_type": "code",
   "id": "9f014cb6-1f01-432e-a269-7c674b6c8fbe",
   "metadata": {
    "language": "python",
    "name": "plot_demand",
    "collapsed": false
   },
   "outputs": [],
   "source": "df_demand.to_pandas().plot.bar(x=\"DAY_OF_WEEK\", y=[\"NEW_PRICE_DEMAND\",\"CURRENT_PRICE_DEMAND\"])",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell44",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "## Calculate Profit and Demand Lift\nProfit is the total profit (average basket profit sold with Chicken Burritos + menu item profit) multiplied by demand.\n\n**Value:** Push-down calculations in Snowflake of key metrics using model results.",
   "id": "724faa9c-4940-471d-a13a-b2afeaed7951"
  },
  {
   "cell_type": "code",
   "id": "c1c27603-202c-4e80-a4a5-3694f5ebf4ab",
   "metadata": {
    "language": "python",
    "name": "show_weekly_demand_and_profit_lift",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Demand lift\ndemand_lift = df_demand.select(\n    F.round(((F.sum(\"new_price_demand\") - F.sum(\"current_price_demand\"))\n            / F.sum(\"current_price_demand\"))* 100, 1)).collect()[0][0]\n\n# Profit lift\nprofit_lift = (\n    df_demand.with_column(\n        \"new_price_profit\",\n        F.col(\"new_price_demand\")\n        * (F.col(\"new_price\") - F.col(\"item_cost\") + F.col(\"average_basket_profit\")))\n    .select(\n        F.round(((F.sum(\"new_price_profit\") - F.sum(\"current_price_profit\"))\n                  / F.sum(\"current_price_profit\")) * 100, 1)).collect()[0][0]\n)\n\nprint(\"Total Weekly Demand Lift (%)\", demand_lift)\nprint(\"Total Weekly Profit Lift (%)\", profit_lift)",
   "execution_count": null
  },
  {
   "metadata": {
    "name": "cell45",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "We see that the Wednesday promotion increases demand, but has a minimal effect on profit. We need a better process for brand managers to submit their pricing and use the ML-driven pricing recommendations to drive their decisions around promotions.\n\n**Value**: Streamlit in Snowflake (SiS) provides an easy way to build user interfaces for allowing business users to interact with ML models and data in Snowflake. Plus, deployment is seamless and governed.",
   "id": "de08a5ff-286c-49f6-8705-0e56b3f89aba"
  },
  {
   "metadata": {
    "name": "cell46",
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": "**Demo: \t\tTasty Bytes - Price Optimization**\nScript: \t    tasty_bytes_price_recommendations.ipynb  \t\nCreate Date:    2023-06-08\nAuthor:         Marie Coolsaet\n\n**Description:**\n\nDeployed item-level demand forecasts are used to analyze monthly pricing set by Tasty Bytes's brand managers.\n\n**Summary of Changes:**\n\n| Date(yyyy-mm-dd) | Author         | Comments                        |\n| :---             | :---           | :---                                                  |\n| 2023-06-068      | Marie Coolsaet | Release 1 | \n| 2024-03-07       | Shriya Rai | Update with Snowpark ML |",
   "id": "9635bc25-79b7-490b-bb03-9e3d44653d33"
  }
 ]
}